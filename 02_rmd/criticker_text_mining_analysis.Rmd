---
title: "Criticker Reviews Text Mining Analysis"
author: "Chris Selig"
date: "`r Sys.Date()`"
output: 
    html_document: default
    pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = FALSE,
    include = TRUE,
    warning = FALSE,
    message = FALSE,
    dpi = 300
    )
```

```{r loadLibraries}
# Data load
library(XML)
library(data.table)

# General processing
library(tidyverse)

# Formatting
library(scales)
library(ggrepel)

# Extra charting capabilities
library(gt) # create pretty tables
library(ggraph) # create network graphs
library(wordcloud) # create wordclouds

# Analytics
library(tidytext)
library(igraph)
library(widyr)
library(sentimentr)

# Themes
library(ggthemes)
```

```{r loadScripts}
source('../01_scripts/01_data_processing.R')
source('../01_scripts/02_plotting.R')
```

```{r loadFonts}
windowsFonts(memphis=windowsFont("memphis"))
windowsFonts(ArnoProLightDisplay = windowsFont("Arno Pro Light Display"))
```

# Summary
My brother and a couple of his friends provide movie reviews on the site [Criticker.com](https://www.criticker.com/). I thought it would be fun to provide them with a text analysis of their movie reviews. What follows is that review.

The workflow for this project will start with an exploration of the data, 

The code for this analysis can be on my [Github page](https://github.com/chrisselig/movie_review_text_analysis).

You can also find the three reviewers pages here:

1. [Tyler](https://www.criticker.com/profile/Obdurate/)

2. [Zach](https://www.criticker.com/profile/XakkMaster/)

3. [Justin](https://www.criticker.com/profile/TheRealJ_Ro/)

## The Data
The dataset is simple. It is comprised of an xml export for each user and after cleaning, there are only two columns:

1. quote: The movie review

2. reviewer: the first name of the reviewer

The following steps were taken to preprocess the data. 

First, I only cared about movies that actually had a review so for any movies that just had a score were removed.

Second, the movie reviews were tokenized. Tokenization means taking a review and breaking it out into one word per row. This makes the dataset significantly longer.

Finally, a significant amount of words in the English language are not useful for sentiment analysis or prediction. Some examples are 'and' and 'the'. These words are otherwise known as stop words and have been removed from the dataset.

## Exploratory Analysis
A good place to start with any analysis is with an exploration of the data. 

First, a quick look at how many reviews have been created and how many total words have been written for each reviewer.

```{r summaryTable}

total_word_count <- reviews %>% 
    unnest_tokens(word, quote) %>% 
    group_by(reviewer) %>% 
    summarise(`Total Word Count` = n())

min_word_count <- reviews %>%
    group_by(reviewer) %>%
    mutate(wc = str_count(quote, '\\w+')) %>% 
    select(-quote) %>% 
    summarize(`Shortest Review` = min(wc))

max_word_count <- reviews %>%
    group_by(reviewer) %>%
    mutate(wc = str_count(quote, '\\w+')) %>% 
    select(-quote) %>% 
    summarize(`Longest Review` = max(wc))

word_count_without_stop_words <- reviews %>% 
    unnest_tokens(word, quote) %>% 
    anti_join(stop_words) %>% 
    group_by(reviewer) %>% 
    summarise(`Word Count \n excl Stop Words` = n())

review_count_gt <- reviews %>%
    group_by(reviewer) %>% 
    summarize(
        `Review Count` = n()
    ) %>% 
    left_join(total_word_count) %>% 
    left_join(word_count_without_stop_words) %>% 
    mutate(`Avg Word Count per Review` = round(`Total Word Count`/`Review Count`,1)) %>% 
    left_join(min_word_count) %>% 
    left_join(max_word_count) %>% 
    rename(Reviewer = reviewer) %>% 
    mutate(`% of Stop Words` = 1-round(`Word Count \n excl Stop Words`/`Total Word Count`,2)) %>% 
    # Create gt table for the plot
    gt() %>%
    tab_header(
        title = "Review Summary",
        subtitle = "Summary Statistics for Each Reviewer"
    ) %>% 
    # Formatting values in columns
    fmt_number(
        columns = vars(`Review Count`, `Total Word Count`, `Word Count \n excl Stop Words`,`Avg Word Count per Review`),
        decimals = 0
    ) %>% 
    fmt_percent(
        columns = vars(`% of Stop Words`),
        decimals = 1
    ) %>% 
    # Center columns
    cols_align(
        align = "center",
        columns = everything()
    ) %>%   
    # Title Styling
    tab_style(
        style = list(
            cell_text(
                font = "memphis",
                color = "#4C586F")
        ),
        locations = cells_title(groups = c("title"))
    ) %>% 
    # Change font of different table areas
    tab_style(
        style = list(
            cell_text(font = "Arno Pro Light Display")
        ),
        locations = list(
            cells_body(columns = TRUE, rows = TRUE),
            # cells_summary(groups = TRUE, columns = TRUE, rows = TRUE),
            cells_column_labels(columns = everything()),
            cells_row_groups()
        )
    ) %>% 
    tab_options(
        # Remove top border
        table.border.top.color = "white",
        column_labels.vlines.color = 'white',
        # Change the style of horizontal lines (hlines)
        table_body.hlines.style = "dashed",
        # Change the color of the hlines
        table_body.hlines.color = '#CBC5C1'
    )

review_count_gt

```

Some observations from the summary statistics. Zach has the fewest total reviews while Tyler has the most. All 3 reviewers have the same average word count per reviews. All three have a one word review while the longest reviews for each are all around the same word count. Finally, Zach has the lowest percentage of stop words among the three leading me to believe his reviews may be more concise than the other two reviewers.

Second, I will take a look at the most common words from all the reviewers combined.

```{r topWords, include=TRUE}
# Set theme for all plots
theme_set(theme_tufte())

tidy_reviews %>% 
    count(word) %>% 
    mutate(
        prop = percent(n/sum(n),accuracy = .1),
        label_text = str_glue('{n} ({prop})')
        ) %>% 
    arrange(desc(n)) %>% 
    head(10) %>%
    bar_charts_func( 
                label = label_text,
                title = 'Most Common Reviewer Words',
                subtitle = 'Shows counts and proportions of the 10 most common words used by all the reviewers',
                xlabel = '',
                ylabel = ''
    )
    
```

Looking at the image above, the top word used across all three reviewers is 'movie', which is not a big surprise. Furthermore, none of the words on the list look out of place for movie reviewers. 

After knowing the top ten words, lets look at the top 10 words for each reviewer.

```{r}
tidy_reviews %>% 
    group_by(reviewer) %>% 
    count(word) %>% 
    top_n(10, n) %>% 
    ungroup() %>% 
    mutate(word = tidytext::reorder_within(word, n,reviewer)) %>%
    mutate(
        prop = percent(n/sum(n),accuracy = .1),
        label_text = str_glue('{n}\n{prop}')
    ) %>% 
    arrange(desc(n)) %>% 
    bar_charts_func(
                    facet = TRUE, 
                    facet_variable = reviewer,
                    label = label_text,
                    title = 'Most Commonly Used Words by Reviewer',
                    subtitle = 'Shows counts and proportions of the 10 most common words used by reviewer',
                    xlabel = '',
                    ylabel = '',
                    hjust = 1.25,
                    size = 1.9)
```

Generally speaking, both Tyler and Justin use some pretty similar words. What is interesting to me is that Zach actually uses 'film' more often than 'movie', which is less popular between Tyler and Justin. Also, Zach likes to talk about scenes more often than both Tyler or Justin.

Previously we only looked at the top 10 words for each reviewer. Below you will find a visual representation (wordcloud) of all words for each reviewer. To read the wordcloud, the largest text is the word appearing most frequently in the reviews. Below are the wordclouds for Justin, Tyler and Zach.

```{r}
set.seed(42)

wc_data <- tidy_reviews %>% 
    group_by(reviewer) %>% 
    count(word) %>% 
    filter(n > 1) %>% 
    ungroup()
    
wc_data %>% 
    filter(reviewer == 'Justin') %>% 
    with(wordcloud(word, freq = n, min.freq = 1, max.words=150, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2")))

title('Justin\'s Words')


```

```{r}
set.seed(42)

wc_data %>% 
    filter(reviewer == 'Tyler') %>% 
    with(wordcloud(word, freq = n, min.freq = 1, max.words=150, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2")))

title('Tyler\'s Words')
```

```{r zachWordCloud}
set.seed(42)

wc_data %>% 
    filter(reviewer == 'Zach') %>% 
    with(wordcloud(word, freq = n, min.freq = 1, max.words=150, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2")))

title('Zach\'s Words')
```

After looking at the top ten words and all the words for each reviewer, we can visually look at the frequencies of all the words by a reviewer compared to another reviewer.

```{r frequencyPlot}
freq_tbl <- tidy_reviews %>% 
    group_by(reviewer) %>% 
    count(reviewer,word) %>% 
    mutate(proportion = n/sum(n)) %>% 
    select(-n) %>% 
    pivot_wider(
        names_from = reviewer,
        values_from = proportion
    ) %>% 
    pivot_longer(
        cols = c(Justin,Zach),
        names_to = 'reviewer',
        values_to = 'proportion'
    )

freq_tbl %>% 
    ggplot(aes(x = proportion, y = Tyler, color = abs(Tyler - proportion))) +
    # Geoms
    geom_abline(color = 'gray40', lty = 2) +
    geom_jitter(alpha = 0.1, size = 2.5,width = 0.3, height = 0.3) +
    geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
    # Formatting
    labs(
        title = 'Word Frequencies by Reviewers',
        subtitle = 'Compares the Frequencies of Words Between Each Reviewer',
        y = 'Tyler', 
        x = NULL 
    ) +
    scale_x_log10(labels = percent_format()) +
    scale_y_log10(labels = percent_format()) +
    scale_color_gradient(limits = c(0,0.001),
                         low = '#CBF8DF', high = '#4C586F') +
    facet_wrap(~reviewer, ncol = 2) +
    theme(
        plot.title = element_text(size = 14, family = "memphis",color = '#4C586F', face = 'bold'),
        plot.subtitle = element_text(hjust = 0.01, size = 11,family = "Arno Pro Light Display"),
        legend.position = 'none'
    ) 
```

Above, the frequencies of each word are plotted against each reviewer. Words along the dotted line have similar frequencies between the two reviewers. So, in the left panel, both Tyler and Justin use "movies", "bad", and "character" with similar frequencies. Words that are farther from the line are words that appear more often in one reviewers text than the other. For example from the right panel, Zach has the words "disney", and "viewer" appear more frequently than in Tyler's while Tyler has "pretty" and "nice" appear more often.

## Relationships Between Words

After looking at the counts and frequency of single words, it is time to look at the relationship between words. The first part of section we will be looking at n-grams. N-grams are sets of adjacent words. For example, bigrams are two words in the text that are adaject and trigrams are three adjacent words. 

Below are the top bigrams for each reviewer.

```{r calcBigrams}
bigrams <- reviews %>% 
    group_by(reviewer) %>% 
    unnest_tokens(bigram, quote, token = 'ngrams', n = 2)

# Separate bigrams so we can remove stop words
bigram_counts <- bigrams %>% 
    separate(bigram, c('word1','word2'), sep = ' ') %>% 
    filter(!word1 %in% stop_words$word) %>% 
    filter(!word2 %in% stop_words$word) %>% 
    count(word1, word2, sort = TRUE)
```

```{r visualizeBigrams}
bigram_counts %>%
    group_by(reviewer) %>%
    top_n(10, n) %>%
    ungroup() %>%
    mutate(word = str_glue('{word1} {word2}')) %>% 
    mutate(word = tidytext::reorder_within(word, n,reviewer)) %>%
    mutate(
        prop = percent(n/sum(n),accuracy = .1),
        label_text = str_glue('{n}\n{prop}')
    ) %>%
    arrange(desc(n)) %>%
    bar_charts_func(
                    facet = TRUE,
                    facet_variable = reviewer,
                    label = label_text,
                    title = 'Most Commonly Used Bigrams by Reviewer',
                    subtitle = 'Shows counts and proportions of the 10 most common bigrams used by each reviewer',
                    xlabel = '',
                    ylabel = '',
                    hjust = 1.25,
                    size = 1.9)
```

Now it is starting to get interesting. I would guess that all three reviewers prefer sci-fi movies because it is the most popular adjacent words in the data sets for them. Zach appears to really like the actor Robert Deniro because he is in the top ten bigrams. Tyler and Justin also appear to watch quite a few horror movies, but Tyler appears to watch more action movies than horror movies. Zach appears to discuss more of the meta parts of films, including special effects, voice acting and physical comedy as well as having a place in his heart for movies with love stories. 

As with individual words, we also want to take a look at not just the top ten bigrams, but also the relationship between all bigrams. We will do this using a network graph. A network graph shows the connections between nodes, which are words in this case. Network graphs are really popular in social media data analytics.

We will look at each reviewer separately, starting with Justin. Note, shape of the graphs do not matter. They are randomly generated each time the code is run so no meaning should be derived from them.


```{r justinNetwork}
network_graph_func(
    bigram_counts,reviewer = 'Justin', num = 4, title = 'Bigrams for Justin',subtitle = 'To keep chart clean, only bigrams with 5 or more appearances are visualized'
)
```

A quick glance at the network graph shows some normal connections, for example around 'movie'. It is connected to typical words related to movies like 'kids', 'horror', and 'comic'. He appears to also like to use phrases such as 'totally worth watching' or 'totally worth checking'. It also appears that he likes to talk about actors because they appear frequently: 'Tom Cruise','Liam Neeson', and 'Martin Lawrence'. I'm also trying to figure out why he talks about 'pro wresting' so much. 

Below is the network graph of Tyler's bigrams. Like Justin, Tyler also likes to mention actor names, like 'Tom Cruise'. The most interesting part (to me) is the cluster around 'movie(s)'. Pretty extinsively, Tyler uses the word 'movie' but not just for talking about the genre, but also his feelings about the movie: 'forgettable', 'pretty', and 'funniest'. 

```{r tylerNetwork}
network_graph_func(
    bigram_counts,reviewer = 'Tyler', num = 4, title = 'Bigrams for Tyler',subtitle = 'To keep chart clean, only bigrams with 5 or more appearances are visualized'
)
```

Finally, we move onto Zach's bigrams. The first thing you probably noticed is that Zach's network graph is quite a bit more sparse than the others. That is because he has the lowest review count and so there is a smaller mount of adjacent words. Zach's reviews appear to frequently discuss the characters, whether they were main or support and if they were memorable. He also apparently likes true love stories. 

```{r zachNetwork}
network_graph_func(
    bigram_counts,reviewer = 'Zach', num = 4, title = 'Bigrams for Zach',subtitle = 'To keep chart clean, only bigrams with 5 or more appearances are visualized'
)
```

## Sentiment Analysis

Now that we have taken a look at the individual and adjacent words, it is time to look at the sentiment of the movie reviews. We are not going to look at the sentiment of individual words because it is a bit too primitive and the English language syntactically complex and lexically rich. 

The algorithm I will use is a bit better than sentiment by word. It uses "valence shifters" that help adjust the sentiment score. For example, if you do sentiment analysis on the single word "happy", the score is positive. Obviously though, if the phrase is "not happy" it is no longer positive but a single word sentiment analysis would not pick that up. The valence shifters will help adjust "not happy" to negative or at least less positive.

```{r}
sentence_sentiment <- reviews %>%
    group_by(reviewer) %>% 
    unnest_tokens(sentence, quote, token = 'sentences') %>% 
    ungroup() %>% 
    get_sentences(quote) %>% 
    # sentimentr::sentiment()
    sentiment_by(quote, list(reviewer, movie))

```



## Review Classificiation
Finally, it is time to do a little classification modeling. The end goal here will be to see if we can correctly classify the reviews into the categories for each reviewer. 

Bucket Stuff:

Justin:
Horrific: 0 - 15
Bad: 16 - 34
Average: 35 - 69
Good: 70 - 79
Great: 80 - 89
Exceptional: 90 - 100

Zach:
Horrible: 0 - 24
Bad: 25 - 49
Average: 50 - 69
Good: 70 - 79
Great: 80 - 89
Exceptional: 90 - 100


Tyler:
Horrible: 0 - 24
Bad: 25 - 49
Average: 50 - 69
Good: 70 - 79
Great: 80 - 89
Exceptional: 90 - 100


This is how each reviewer categorizes their review score.